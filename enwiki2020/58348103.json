{
    "id": "58348103",
    "text": "Sentence embedding is the collective name for a set of techniques in natural language processing (NLP) where sentences are mapped to vectors of real numbers Paper Summary: Evaluation of sentence embeddings in downstream and linguistic probing tasks Oren Barkan, Noam Razin, Itzik Malkiel, Ori Katz, Avi Caciularu, Noam Koenigstein. \"Scalable Attentive Sentence-Pair Modeling via Distilled Sentence Embedding\". AAAI 2020; arxiv:1908.05161. The Current Best of Universal Word Embeddings and Sentence Embeddings Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope: \u201cUniversal Sentence Encoder\u201d, 2018; arXiv:1803.11175. Ledell Wu, Adam Fisch, Sumit Chopra, Keith Adams, Antoine Bordes: \u201cStarSpace: Embed All The Things!\u201d, 2017; arXiv:1709.03856. Sanjeev Arora, Yingyu Liang, and Tengyu Ma. \"A simple but tough-to-beat baseline for sentence embeddings.\", 2016; openreview:SyK00v5xx. Mircea Trifan, Bogdan Ionescu, Cristian Gadea, and Dan Ionescu. \"A graph digital signal processing method for semantic analysis.\" In Applied Computational Intelligence and Informatics (SACI), 2015 IEEE 10th Jubilee International Symposium on, pp. 187-192. IEEE, 2015; ieee:7208196. Pierpaolo Basile, Annalina Caputo, and Giovanni Semeraro. \"A study on compositional semantics of words in distributional spaces.\" In Semantic Computing (ICSC), 2012 IEEE Sixth International Conference on, pp. 154-161. IEEE, 2012; ieee:6337099 . . == Application == Sentence embedding is used by the deep learning software libraries PyTorch and TensorFlow == Evaluation == A way of testing sentence encodings is to apply them on Sentences Involving Compositional Knowledge (SICK) corpusMarco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zamparelli. \"A SICK cure for the evaluation of compositional distributional semantic models.\" In LREC, pp. 216-223. 2014 . for both entailment (SICK-E) and relatedness (SICK-R). In Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault: \u201cSupervised Learning of Universal Sentence Representations from Natural Language Inference Data\u201d, 2017; arXiv:1705.02364. the best results are obtained using a BiLSTM network trained on the Stanford Natural Language Inference (SNLI) Corpus. The Pearson correlation coefficient for SICK-R is 0.885 and the result for SICK-E is 86.3. A slight improvement over previous scores is presented in Sandeep Subramanian, Adam Trischler, Yoshua Bengio: \u201cLearning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning\u201d, 2018; arXiv:1804.00079.: SICK-R: 0.888 and SICK-E: 87.8 using a concatenation of bidirectional Gated recurrent unit. == See also == * Distributional semantics * Word embedding == External links == InferSent sentence embeddings and training code Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning == References == Category:Language modeling Category:Artificial neural networks Category:Natural language processing Category:Computational linguistics ",
    "title": "Sentence embedding"
}