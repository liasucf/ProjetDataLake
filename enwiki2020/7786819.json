{
    "id": "7786819",
    "text": "In statistics, the generalized linear array model (GLAM) is used for analyzing data sets with array structures. It based on the generalized linear model with the design matrix written as a Kronecker product. == Overview == The generalized linear array model or GLAM was introduced in 2006. Such models provide a structure and a computational procedure for fitting generalized linear models or GLMs whose model matrix can be written as a Kronecker product and whose data can be written as an array. In a large GLM, the GLAM approach gives very substantial savings in both storage and computational time over the usual GLM algorithm. Suppose that the data \\mathbf Y is arranged in a d-dimensional array with size n_1\\times n_2\\times\\ldots\\times n_d; thus, the corresponding data vector \\mathbf y = \\textbf{vec}(\\mathbf Y) has size n_1n_2n_3\\cdots n_d. Suppose also that the design matrix is of the form :\\mathbf X = \\mathbf X_d\\otimes\\mathbf X_{d-1}\\otimes\\ldots\\otimes\\mathbf X_1. The standard analysis of a GLM with data vector \\mathbf y and design matrix \\mathbf X proceeds by repeated evaluation of the scoring algorithm : \\mathbf X'\\tilde{\\mathbf W}_\\delta\\mathbf X\\hat{\\boldsymbol\\theta} = \\mathbf X'\\tilde{\\mathbf W}_\\delta\\tilde{\\mathbf z} , where \\tilde{\\boldsymbol\\theta} represents the approximate solution of \\boldsymbol\\theta, and \\hat{\\boldsymbol\\theta} is the improved value of it; \\mathbf W_\\delta is the diagonal weight matrix with elements : w_{ii}^{-1} = \\left(\\frac{\\partial\\eta_i}{\\partial\\mu_i}\\right)^2\\text{var}(y_i), and :\\mathbf z = \\boldsymbol\\eta + \\mathbf W_\\delta^{-1}(\\mathbf y - \\boldsymbol\\mu) is the working variable. Computationally, GLAM provides array algorithms to calculate the linear predictor, : \\boldsymbol\\eta = \\mathbf X \\boldsymbol\\theta and the weighted inner product : \\mathbf X'\\tilde{\\mathbf W}_\\delta\\mathbf X without evaluation of the model matrix \\mathbf X . ===Example=== In 2 dimensions, let \\mathbf X = \\mathbf X_2\\otimes\\mathbf X_1, then the linear predictor is written \\mathbf X_1 \\boldsymbol\\Theta \\mathbf X_2' where \\boldsymbol\\Theta is the matrix of coefficients; the weighted inner product is obtained from G(\\mathbf X_1)' \\mathbf W G(\\mathbf X_2) and \\mathbf W is the matrix of weights; here G(\\mathbf M) is the row tensor function of the r \\times c matrix \\mathbf M given by :G(\\mathbf M) = (\\mathbf M \\otimes \\mathbf 1') \\circ (\\mathbf 1' \\otimes \\mathbf M) where \\circ means element by element multiplication and \\mathbf 1 is a vector of 1's of length c. On the other hand, the row tensor function G(\\mathbf M) of the r \\times c matrix \\mathbf M is the example of Face-splitting product of matrices, which was proposed by Vadym Slyusar in 1996: : \\mathbf{M} \\bull \\mathbf{M} = (\\mathbf {M} \\otimes \\mathbf {1}^\\textsf{T}) \\circ (\\mathbf {1}^\\textsf{T} \\otimes \\mathbf {M}), where \\bull means Face-splitting product. These low storage high speed formulae extend to d-dimensions. ==Applications== GLAM is designed to be used in d-dimensional smoothing problems where the data are arranged in an array and the smoothing matrix is constructed as a Kronecker product of d one- dimensional smoothing matrices. ==References== Category:Regression models Array model ",
    "title": "Generalized linear array model"
}