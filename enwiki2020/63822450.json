{
    "id": "63822450",
    "text": "The swish function is a mathematical function defined as follows: : \\operatorname{swish}(x):=x\\times\\operatorname{sigmoid}(\\beta x)=\\frac{x}{1+e^{-\\beta x}}. where \u03b2 is either constant or a trainable parameter depending on the model. For \u03b2=1, the function becomes equivalent to the Sigmoid-weighted Linear Unit (SiL) function used in reinforcement learning, whereas for \u03b2=0, the functions turns into the scaled linear function f(x)=x/2. With \u03b2\u2192\u221e, the sigmoid component approaches a 0-1 function, so swish becomes like the ReLU function. Thus, it can be viewed as a smoothing function which nonlinearly interpolates between a linear and the ReLU function. ==Applications== In 2017, after performing analysis on ImageNet data, researchers from Google alleged that using the function as an activation function in artificial neural networks improves the performance, compared to ReLU and sigmoid functions. It is believed that one reason for the improvement is that the swish function helps alleviate the vanishing gradient problem during backpropagation. ==References== Category:Functions and mappings Category:Artificial neural networks ",
    "title": "Swish function"
}