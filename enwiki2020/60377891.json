{
    "id": "60377891",
    "text": "Concurrent accesses to the same hash table. A concurrent hash table (concurrent hash map) is an implementation of hash tables allowing concurrent access by multiple threads using a hash function. Concurrent hash tables thus represent a key concurrent data structure for use in concurrent computing which allow multiple threads to more efficiently cooperate for a computation among shared data. Due to the natural problems associated with concurrent access - namely contention - the way and scope in which the table can be concurrently accessed differs depending on the implementation. Furthermore, the resulting speed up might not be linear with the amount of threads used as contention needs to be resolved, producing processing overhead. There exist multiple solutions to mitigate the effects of contention, that each preserve the correctness of operations on the table. As with their sequential counterpart, concurrent hash tables can be generalized and extended to fit broader applications, such as allowing more complex data types to be used for keys and values. These generalizations can however negatively impact performance and should thus be chosen in accordance to the requirements of the application. ==Concurrent hashing== When creating concurrent hash tables, the functions accessing the table with the chosen hashing algorithm need to be adapted for concurrency by adding a conflict resolution strategy. Such a strategy requires managing accesses in a way such that conflicts caused by them do not result in corrupt data, while ideally increasing their efficiency when used in parallel. Herlihy and Shavit describe how the accesses to a hash table without such a strategy - in its example based on a basic implementation of the Cuckoo hashing algorithm - can be adapted for concurrent use. Fan et al. further describe a table access scheme based on cuckoo hashing that is not only concurrent, but also keeps the space efficiency of its hashing function while also improving cache locality as well as the throughput of insertions. When hash tables are not bound in size and are thus allowed to grow/shrink when necessary, the hashing algorithm needs to be adapted to allow this operation. This entails modifying the used hash function to reflect the new key-space of the resized table. A concurrent growing algorithm is described by Maier et al. Mega-KVZhang, Kai; Wang, Kaibo; Yuan, Yuan; Guo, Lei; Lee, Rubao; and Zhang, Xiaodong (2015). \"Mega-KV: a case for GPUs to maximize the throughput of in-memory key-value stores\".Proceedings of the VLDB Endowment, Vol. 8, No. 11, 2015. is a high performance key-value store system, where the cuckoo hashing is used and the KV indexing is massively parallelized in batch mode by GPU. With further optimizations of GPU acceleration by NVIDIA and Oak Ridge National Lab, Mega-KV was pushed to another high record of the throughput in 2018 (up to 888 millions of key-value operations per second).Chu, Ching-Hsing; Potluri, Sreeram; Goswami, Anshuman; Venkata, Manjunath Gorentla; Imam, Neenaand; and Newburn, Chris J. (2018) \"Designing High-performance in-memory key-value operations with persistent GPU kernels and OPENSHMEM\".. ==Contention handling== Concurrent accesses causing contention (marked in red). As with any concurrent data structure, concurrent hash tables suffer from a variety of problems known in the field of concurrent computing as a result of contention. Examples for such are the ABA problem, race conditions, and deadlocks. The extent in which these problems manifest or even occur at all depends on the implementation of the concurrent hash table; specifically which operations the table allows to be run concurrently, as well as its strategies for mitigating problems associated with contention. When handling contention, the main goal is the same as with any other concurrent data structure, namely ensuring correctness for every operation on the table. At the same time, it should naturally be done in such a way as to be more efficient than a sequential solution when used concurrently. This is also known as concurrency control. ===Atomic instructions=== Using atomic instructions such as compare-and-swap or fetch-and-add, problems caused by contention can be reduced by ensuring that an access is completed before another access has the chance to interfere. Operations such as compare-and- swap often present limitations as to what size of data they can handle, meaning that the types of keys and values of a table have to be chosen or converted accordingly. Using so called Hardware Transactional Memory (HTM), table operations can be thought of much like database transactions, ensuring atomicity. An example of HTM in practice are the Transactional Synchronization Extensions. ===Locking=== With the help of locks, operations trying to concurrently access the table or values within it can be handled in a way that ensures correct behavior. This can however lead to negative performance impacts, in particular when the locks used are too restrictive, thus blocking accesses that would otherwise not contend and could execute without causing any problems. Further considerations have to be made to avoid even more critical problems that threaten correctness, as with livelocks, deadlocks or starvation. ===Phase concurrency=== Concurrent accesses grouped into distinct phases. A phase concurrent hash table groups accesses by creating phases in which only one type of operation is allowed (i.e. a pure write-phase), followed by a synchronization of the table state across all threads. A formally proven algorithm for this is given by Shun and Blelloch. ===Read- Copy-Update=== Widely used within the Linux kernel, read-copy-update (RCU) is especially useful in cases where the number of reads far exceeds the number of writes. ==Applications== Naturally, concurrent hash tables find application wherever sequential hash tables are useful. The advantage that concurrency delivers herein lies within the potential speedup of these use-cases, as well as the increased scalability. Considering hardware such as multi-core processors that become increasingly more capable of concurrent computation, the importance of concurrent data structures within these applications grow steadily. ==Performance Analysis== Maier et al. perform a thorough analysis on a variety of concurrent hash table implementations, giving insight into the effectiveness of each in different situations that are likely to occur in real use-cases. The most important findings can be summed up as the following: {| class=\"wikitable\" |- ! rowspan=\"2\" scope=\"col\" style=\"width: 75pt;\" | Operation ! colspan=\"2\" scope=\"col\" style=\"width: 100pt;\" | Contention ! rowspan=\"2\" scope=\"col\" style=\"width: 450pt;\" | Notes |- ! Low ! High |- | style=\"text-align:center;\" | `find` || || || Very high speedups both when successful and unsuccessful unique finds, even with very high contention |- | style=\"text-align:center;\" | `insert` || || || High speedups reached, high contention becomes problematic when keys can hold more than one value (otherwise inserts are simply discarded if key already exists) |- | style=\"text-align:center;\" | `update` || || || Both overwrites and modifications of existing values reach high speedups when contention is kept low, otherwise performs worse than sequential |- | style=\"text-align:center;\" | `delete` || || || Phase concurrency reached highest scalability; Fully concurrent implementations where `delete` uses `update` with dummy-elements were closely behind |} As expected low contention leads to positive behavior across every operation, whereas high contention becomes problematic when it comes to writing. The latter however is a problem of high contention in general, wherein the benefit of concurrent computation is negated due to the natural requirement for concurrency control restricting contending accesses. The resulting overhead causes worse performance than that of the ideal sequential version. In spite of this, concurrent hash tables still prove invaluable even in such high contention scenarios when observing that a well- designed implementation can still achieve very high speedups by leveraging the benefits of concurrency to read data concurrently. However, real use-cases of concurrent hash tables are often not simply sequences of the same operation, but rather a mixture of multiple types. As such, when a mixture of `insert` and `find` operations is used the speedup and resulting usefulness of concurrent hash tables become more obvious, especially when observing `find` heavy workloads. Ultimately the resulting performance of a concurrent hash table depends on a variety of factors based upon its desired application. When choosing the implementation, it is important to determine the necessary amount of generality, contention handling strategies and some thoughts on whether the size of the desired table can be determined in advance or a growing approach must be used instead. ==Implementations== * Since Java 1.5, concurrent hash maps are provided based upon concurrent map interface.Java ConcurrentHashMap documentation * libcuckoo provides concurrent hash tables for C/C++ allowing concurrent reads and writes. The library is available on GitHub.GitHub repository for libcuckoo * Threading Building Blocks provide concurrent unordered maps for C++ which allow concurrent insertion and traversal and are kept in a similar style to the C++11 `std::unordered_map` interface. Included within are the concurrent unordered multimaps, which allow multiple values to exist for the same key in a concurrent unordered map.Threading Building Blocks concurrent_unordered_map and concurrent_unordered_multimap documentation Additionally, concurrent hash maps are provided which build upon the concurrent unordered map and further allow concurrent erasure and contain built-in locking.Threading Building Blocks concurrent_hash_map documentation * growt provides concurrent growing hash tables for C++ on the basis of the so- called folklore implementation. Based on this non-growing implementation, a variety of different growing hash tables is given. These implementations allow for concurrent reads, inserts, updates (notably updating values based on the current value at the key) and removals (based upon updating using tombstones). Beyond that, variants on the basis of Intel TSX are provided. The library is available on GitHub. GitHub repository for growt * folly provides concurrent hash tablesGitHub page for implementation of concurrent hash maps in folly for C++14 and later ensuring wait-free readers and lock-based, sharded writers. As stated on its GitHub page, this library provides useful functionality for Facebook.GitHub repository for folly * Junction provides several implementations of concurrent hash tables for C++ on the basis of atomic operations to ensure thread-safety for any given member function of the table. The library is available on GitHub.GitHub repository for Junction ==See Also== * Parallel computing * Liveness * Ctrie ==References== ==Further reading== * Category:Hash based data structures Category:Concurrent computing ",
    "title": "Concurrent hash table"
}