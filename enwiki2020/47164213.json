{
    "id": "47164213",
    "text": "One of the application of Student's t-test is to test the location of one sequence of independent and identically distributed random variables. If we want to test the locations of multiple sequences of such variables, \u0160id\u00e1k correction should be applied in order to calibrate the level of the Student's t-test. Moreover, if we want to test the locations of nearly infinitely many sequences of variables, then \u0160id\u00e1k correction should be used, but with caution. More specifically, the validity of \u0160id\u00e1k correction depends on how fast the number of sequences goes to infinity. ==Introduction== Suppose we are interested in different hypotheses, H_{1},...,H_{m} , and would like to check if all of them are true. Now the hypothesis test scheme becomes : H_{null} : all of H_{i} are true; : H_{alternative}: at least one of H_{i} is false. Let \\alpha be the level of this test (the type-I error), that is, the probability that we falsely reject H_{null} when it is true. We aim to design a test with certain level \\alpha . Suppose when testing each hypothesis H_{i}, the test statistic we use is t_{i}. If these t_{i}'s are independent, then a test for H_{null} can be developed by the following procedure, known as \u0160id\u00e1k correction. :Step 1, we test each of null hypotheses at level 1-(1-\\alpha)^\\frac{1}{m} . :Step 2, if any of these null hypotheses is rejected, we reject H_{null} . ==Finite case== For finitely many t-tests, suppose Y_{ij}=\\mu_{i}+\\epsilon_{ij}, i=1,...,N, j=1,...,n, where for each , \\epsilon_{i1},...,\\epsilon_{in} are independently and identically distributed, for each \\epsilon_{1j},...,\\epsilon_{Nj} are independent but not necessarily identically distributed, and \\epsilon_{ij} has finite fourth moment. Our goal is to design a test for H_{null}: \\mu_{i}=0, \\forall i=1,...,N with level . This test can be based on the t-statistic of each sequences, that is, : t_{i}=\\frac{\\bar{Y}_{i}}{S_{i}/\\sqrt{n}}, where: : \\bar{Y}_{i}=\\frac{1}{n}\\sum_{j=1}^{n}Y_{ij}, \\qquad S_{i}^{2}=\\frac{1}{n}\\sum_{j=1}^{n}(Y_{ij}-\\bar{Y}_{i})^{2}. Using \u0160id\u00e1k correction, we reject H_{null} if any of the t-tests based on the t-statistics above reject at level 1-(1-\\alpha)^{\\frac{1}{N}}. More specifically, we reject H_{null} when : \\exists i \\in \\\\{1,\\ldots,N\\\\} : |t_{i}|> \\zeta_{\\alpha,N}, where : P(|Z|>\\zeta_{\\alpha,N})=1-(1-\\alpha)^{\\frac{1}{N}}, \\qquad Z\\sim N(0,1) The test defined above has asymptotic level , because : \\begin{align} \\text{level} &= P_{null} \\left (\\text{reject } H_{null} \\right) \\\\\\ &= P_{null} \\left(\\exists i \\in \\\\{1,\\ldots,N\\\\} : |t_{i}|>\\zeta_{\\alpha,N} \\right ) \\\\\\ &= 1-P_{null} \\left (\\forall i \\in \\\\{1,\\ldots,N\\\\} : |t_{i}|\\leq\\zeta_{\\alpha,N} \\right ) \\\\\\ &=1-\\prod_{i=1}^{N}P_{null} \\left (|t_{i}|\\leq\\zeta_{\\alpha,N} \\right ) \\\\\\ &\\to 1-\\prod_{i=1}^{N}P \\left (|Z_{i}|\\leq\\zeta_{\\alpha,N} \\right ) && Z_{i}\\sim N(0,1) \\\\\\ &=\\alpha \\end{align} ==Infinite case== In some cases, the number of sequences, N , increase as the data size of each sequences, n , increase. In particular, suppose N(n)\\rightarrow \\infty \\text{ as } n \\rightarrow \\infty . If this is true, then we will need to test a null including infinitely many hypotheses, that is : H_{null}: \\text{ all of } H_{i} \\text{ are true, } i=1,2,.... To design a test, \u0160id\u00e1k correction may be applied, as in the case of finitely many t-test. However, when N(n)\\rightarrow \\infty \\text{ as } n\\rightarrow \\infty, the \u0160id\u00e1k correction for t-test may not achieve the level we want, that is, the true level of the test may not converges to the nominal level \\alpha as n goes to infinity. This result is related to high-dimensional statistics and is proven by Fan, Hall and Yao (2007). Specifically, if we want the true level of the test converges to the nominal level \\alpha , then we need a restraint on how fast N(n)\\rightarrow \\infty . Indeed, * When all of \\epsilon_{ij} have distribution symmetric about zero, then it is sufficient to require \\log N = o (n^{1/3}) to guarantee the true level converges to \\alpha . * When the distributions of \\epsilon_{ij} are asymmetric, then it is necessary to impose \\log N = o(n^{1/2}) to ensure the true level converges to \\alpha . * Actually, if we apply bootstrapping method to the calibration of level, then we will only need \\log N = o (n^{1/3}) even if \\epsilon_{ij} has asymmetric distribution. The results above are based on Central Limit Theorem. According to Central Limit Theorem, each of our t-statistics t_{i} possesses asymptotic standard normal distribution, and so the difference between the distribution of each t_{i} and the standard normal distribution is asymptotically negligible. The question is, if we aggregate all the differences between the distribution of each t_{i} and the standard normal distribution, is this aggregation of differences still asymptotically ignorable? When we have finitely many t_{i} , the answer is yes. But when we have infinitely many t_{i} , the answer some time becomes no. This is because in the latter case we are summing up infinitely many infinitesimal terms. If the number of the terms goes to infinity too fast, that is, N(n) \\rightarrow \\infty too fast, then the sum may not be zero, the distribution of the t-statistics can not be approximated by the standard normal distribution, the true level does not converges to the nominal level \\alpha , and then the \u0160id\u00e1k correction fails. ==See also == * \u0160id\u00e1k correction * Multiple comparisons * Bonferroni correction * Familywise error rate * Closed testing procedure ==Notes== ==References== * Category:Multiple comparisons ",
    "title": "\u0160id\u00e1k correction for t-test"
}