{
    "id": "847558",
    "text": "In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa). The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabeling one as another). It is a special kind of contingency table, with two dimensions (\"actual\" and \"predicted\"), and identical sets of \"classes\" in both dimensions (each combination of dimension and class is a variable in the contingency table). __TOC__ ==Example== Given a sample of 13 pictures, 8 of cats and 5 of dogs, where cats belong to class 1 and dogs belong to class 0, :actual = [1,1,1,1,1,1,1,1,0,0,0,0,0], assume that a classifier that distinguishes between cats and dogs is trained, and we take the 13 pictures and run them through the classifier, and the classifier makes 8 accurate predictions and misses 5: 3 cats wrongly predicted as dogs (first 3 predictions) and 2 dogs wrongly predicted as cats (last 2 predictions). :prediction = [0,0,0,1,1,1,1,1,0,0,0,1,1] With these two labelled sets (actual and predictions) we can create a confusion matrix that will summarize the results of testing the classifier: {| |- | {| class=\"wikitable\" style=\"border:none; float:left; margin-top:0; text-align:center;\" !style=\"background:white; border:none;\" colspan=\"2\" rowspan=\"2\"| !colspan=\"2\" style=\"background:none;\"| Actual class |- !Cat !Dog |- !rowspan=\"2\" style=\"height:6em;background:none;\"| Predicted class !Cat |5 |2 |- !Dog |3 |3 |- |} | |} In this confusion matrix, of the 8 cat pictures, the system judged that 3 were dogs, and of the 5 dog pictures, it predicted that 2 were cats. All correct predictions are located in the diagonal of the table (highlighted in bold), so it is easy to visually inspect the table for prediction errors, as they will be represented by values outside the diagonal. In abstract terms, the confusion matrix is as follows: {| |- | {| class=\"wikitable\" style=\"border:none; float:left; margin-top:0; text- align:center\" !style=\"background:white; border:none;\" colspan=\"2\" rowspan=\"2\"| !colspan=\"2\" style=\"background:none;\"| Actual class |- !P !N |- !rowspan=\"2\" style=\"height:6em;background:none;\"| Predicted class !P |TP |FP |- !N |FN |TN |- |} | |} where: P = Positive; N = Negative; TP = True Positive; FP = False Positive; TN = True Negative; FN = False Negative. ==Table of confusion== In predictive analytics, a table of confusion (sometimes also called a confusion matrix) is a table with two rows and two columns that reports the number of false positives, false negatives, true positives, and true negatives. This allows more detailed analysis than mere proportion of correct classifications (accuracy). Accuracy will yield misleading results if the data set is unbalanced; that is, when the numbers of observations in different classes vary greatly. For example, if there were 95 cats and only 5 dogs in the data, a particular classifier might classify all the observations as cats. The overall accuracy would be 95%, but in more detail the classifier would have a 100% recognition rate (sensitivity) for the cat class but a 0% recognition rate for the dog class. F1 score is even more unreliable in such cases, and here would yield over 97.4%, whereas informedness removes such bias and yields 0 as the probability of an informed decision for any form of guessing (here always guessing cat). According to Davide Chicco and Giuseppe Jurman, the most informative metric to evaluate a confusion matrix is the Matthews correlation coefficient (MCC). Assuming the confusion matrix above, its corresponding table of confusion, for the cat class, would be: {| class=\"wikitable\" style=\"border:none; margin-top:0;\" !style=\"background:white; border:none;\" colspan=\"2\" rowspan=\"2\"| !colspan=\"3\" style=\"background:none;\"| Actual class |- !Cat !Non-cat |- !rowspan=\"3\" style=\"height:6em;background:none;\"| Predicted class !Cat |5 True Positives |2 False Positives |- !Non-cat |3 False Negatives |3 True Negatives |- |} The final table of confusion would contain the average values for all classes combined. Let us define an experiment from P positive instances and N negative instances for some condition. The four outcomes can be formulated in a 2\u00d72 confusion matrix, as follows: ==References== Category:Machine learning Category:Statistical classification ",
    "title": "Confusion matrix"
}