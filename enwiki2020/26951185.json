{
    "id": "26951185",
    "text": "Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) is a matrix- free method for finding the largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive definite generalized eigenvalue problem :A x= \\lambda B x, for a given pair (A, B) of complex Hermitian or real symmetric matrices, where the matrix B is also assumed positive-definite. ==Background== Kantorovich in 1948 proposed calculating the smallest eigenvalue \\lambda_1 of a symmetric matrix A by steepest descent using a direction r = Ax-\\lambda (x) x of a scaled gradient of a Rayleigh quotient \\lambda(x) = (x, Ax)/(x, x) in a scalar product (x, y) = x'y, with the step size computed by minimizing the Rayleigh quotient in the linear span of the vectors x and w, i.e. in a locally optimal manner. Samokish proposed applying a preconditioner T to the residual vector r to generate the preconditioned direction w = T r and derived asymptotic, as x approaches the eigenvector, convergence rate bounds. D'yakonov suggested spectrally equivalent preconditioning and derived non-asymptotic convergence rate bounds. Block locally optimal multi-step steepest descent for eigenvalue problems was described in. Local minimization of the Rayleigh quotient on the subspace spanned by the current approximation, the current residual and the previous approximation, as well as its block version, appeared in. The preconditioned version was analyzed in and. ==Main features== * LOBPCG is matrix-free, i.e. does not require storing the coefficient matrix explicitly, but can accesses the matrix by evaluating matrix-vector products. * The costs per iteration and the memory use in LOBPCG are competitive with those of the Lanczos method, computing a single extreme eigenpair. * Linear convergence is theoretically guaranteed and practically observed, since local optimality implies that LOBPCG converges at least as fast as the gradient descent method. In numerical tests, LOBPCG typically shows no super-linear convergence. * LOBPCG blocking allows utilizing highly efficient matrix-matrix operations, e.g., BLAS 3. * LOBPCG can directly take advantage of preconditioning, in contrast to the Lanczos method. LOBPCG allows variable and non-symmetric as well as fixed and positive definite preconditioning. * LOBPCG allows warm starts and computes an approximation to the eigenvector on every iteration. It has no numerical stability issues similar to those of the Lanczos method. * LOBPCG is reasonably easy to implement, so many implementations have appeared. * LOBPCG general technology can also be viewed as a particular case of generalized block Davidson diagonalization methods with thick restart, or accelerated block gradient descent with plane-search. * Very large block sizes in LOBPCG become expensive to deal with due to orthogonalizations and the use of the Rayleigh-Ritz method on every iteration. ==Algorithm== ===Single-vector version=== The method performs an iterative maximization (or minimization) of the generalized Rayleigh quotient :\\rho(x) := \\rho(A,B; x) :=\\frac{x^T A x}{x^T B x}, which results in finding largest (or smallest) eigenpairs of A x= \\lambda B x. The direction of the steepest ascent, which is the gradient, of the generalized Rayleigh quotient is positively proportional to the vector :r := Ax - \\rho(x) Bx, called the eigenvector residual. If a preconditioner T is available, it is applied to the residual and gives the vector :w := Tr, called the preconditioned residual. Without preconditioning, we set T := I and so w := r. An iterative method :x^{i+1} := x^i + \\alpha^i T(Ax^i - \\rho(x^i) Bx^i), or, in short, :x^{i+1} := x^i + \\alpha^i w^i,\\, :w^i := Tr^i,\\, :r^i := Ax^i - \\rho(x^i) Bx^i, is known as preconditioned steepest ascent (or descent), where the scalar \\alpha^i is called the step size. The optimal step size can be determined by maximizing the Rayleigh quotient, i.e., :x^{i+1} := \\arg\\max_{y\\in span\\\\{x^i,w^i\\\\}} \\rho(y) (or \\arg\\min in case of minimizing), in which case the method is called locally optimal. To further accelerate the convergence of the locally optimal preconditioned steepest ascent (or descent), one can add one extra vector to the two-term recurrence relation to make it three-term: :x^{i+1} := \\arg\\max_{y\\in span\\\\{x^i,w^i,x^{i-1}\\\\}} \\rho(y) (use \\arg\\min in case of minimizing). The maximization/minimization of the Rayleigh quotient in a 3-dimensional subspace can be performed numerically by the Rayleigh\u2013Ritz method. As the iterations converge, the vectors x^i and x^{i-1} become nearly linearly dependent, making the Rayleigh\u2013Ritz method numerically unstable in the presence of round-off errors. It is possible to substitute the vector x^{i-1} with an explicitly computed difference p^i=x^{i-1}-x^i making the Rayleigh\u2013Ritz method more stable; see. This is a single-vector version of the LOBPCG method\u2014one of possible generalization of the preconditioned conjugate gradient linear solvers to the case of symmetric eigenvalue problems. Even in the trivial case T=I and B=I the resulting approximation with i>3 will be different from that obtained by the Lanczos algorithm, although both approximations will belong to the same Krylov subspace. ===Block version=== Iterating several approximate eigenvectors together in a block in a similar locally optimal fashion, gives the full block version of the LOBPCG. It allows robust computation of eigenvectors corresponding to nearly-multiple eigenvalues. ==Convergence theory and practice== LOBPCG by construction is guaranteed to minimize the Rayleigh quotient not slower than the block steepest gradient descent, which has a comprehensive convergence theory. Every eigenvector is a stationary point of the Rayleigh quotient, where the gradient vanishes. Thus, the gradient descent may slow down in a vicinity of any eigenvector, however, it is guaranteed to either converge to the eigenvector with a linear convergence rate or, if this eigenvector is a saddle point, the iterative Rayleigh quotient is more likely to drop down below the corresponding eigenvalue and start converging linearly to the next eigenvalue below. The worst value of the linear linear convergence rate has been determined and depends on the relative gap between the eigenvalue and the rest of the matrix spectrum and the quality of the preconditioner, if present. For a general matrix, there is evidently no way to predict the eigenvectors and thus generate the initial approximations that always work well. The iterative solution by LOBPCG may be sensitive to the initial eigenvectors approximations, e.g., taking longer to converge slowing down as passing intermediate eigenpairs. Moreover, in theory, one cannot guarantee convergence necessarily to the smallest eigenpair, although the probability of the miss is zero. A good quality random Gaussian function with the zero mean is commonly the default in LOBPCG to generate the initial approximations. To fix the initial approximations, one can select a fixed seed for the random number generator. In contrast to the Lanczos method, LOBPCG rarely exhibits asymptotic superlinear convergence in practice. ==Partial Principal component analysis (PCA) and Singular Value Decomposition (SVD)== LOBPCG can be trivially adopted for computing several largest singular values and the corresponding singular vectors (partial SVD), e.g., for iterative computation of PCA, for a data matrix with zero mean, without explicitly computing the covariance matrix , i.e. in matrix-free fashion. The main calculation is evaluation of a function of the product of the covariance matrix and the block-vector that iteratively approximates the desired singular vectors. PCA needs the largest eigenvalues of the covariance matrix, while LOBPCG is typically implemented to calculate the smallest ones. A simple work- around is to negate the function, substituting for and thus reversing the order of the eigenvalues, since LOBPCG does not care if the matrix of the eigenvalue problem is positive definite or not.LOBPCG at Mathworks LOBPCG for PCA and SVD is implemented in SciPy since revision 1.4.0 LOBPCG in SciPy ==General software implementations== LOBPCG's inventor, Andrew Knyazev, published a reference implementation called Block Locally Optimal Preconditioned Eigenvalue Xolvers (BLOPEX)GitHub BLOPEX with interfaces to PETSc, hypre, and Parallel Hierarchical Adaptive MultiLevel method (PHAML)PHAML BLOPEX interface to LOBPCG. Other implementations are available in, e.g., GNU OctaveOctave linear-algebra function lobpcg, MATLAB (including for distributed or tiling arrays)MATLAB File Exchange function LOBPCG, JavaJava LOBPCG at Google Code, Anasazi (Trilinos)Anasazi Trilinos LOBPCG at GitHub, SLEPcNative SLEPc LOBPCGSLEPc BLOPEX interface to LOBPCG, SciPySciPy sparse linear algebra function lobpcg , Julia Julia LOBPCG at GitHub, MAGMA, Pytorch Pytorch LOBPCG at GitHub, Rust Rust LOBPCG at GitHub, OpenMP and OpenACC, RAPIDS cuGraph RAPIDS cuGraph NVgraph LOBPCG at GitHub and NVIDIA AMGX.NVIDIA AMGX LOBPCG at GitHub LOBPCG is implemented, but not included, in TensorFlow. ==Applications== ===Material sciences=== LOBPCG is implemented in ABINITABINIT Docs: WaveFunction OPTimisation ALGorithm (including CUDA version) and Octopus.Octopus Developers Manual:LOBPCG It has been used for multi-billion size matrices by Gordon Bell Prize finalists, on the Earth Simulator supercomputer in Japan. Recent implementations include TTPY, Platypus\u2010QM, and MFDn. Hubbard model for strongly-correlated electron systems to understand the mechanism behind the superconductivity uses LOBPCG to calculate the ground state of the Hamiltonian on the K computer. There are MATLAB and Julia[https://juliaobserver.com/packages/PWDFT PWDFT Plane wave density functional theory using Julia programming language versions of LOBPCG for Kohn-Sham equations and density functional theory (DFT) using the plain- wave basis. ===Mechanics and fluids=== LOBPCG from BLOPEX is used for preconditioner setup in Multilevel Balancing Domain Decomposition by Constraints (BDDC) solver library BDDCML, which is incorporated into OpenFTL (Open Finite element Template Library) and Flow123d simulator of underground water flow, solute and heat transport in fractured porous media. LOBPCG has been implemented in LS-DYNA. ===Maxwell's equations=== LOBPCG is one of core eigenvalue solvers in PYFEMax and high performance multiphysics finite element software Netgen/NGSolve. LOBPCG from hypre is incorporated into open source lightweight scalable C++ library for finite element methods MFEM, which is used in many projects, including BLAST, XBraid, VisIt, xSDK, the FASTMath institute in SciDAC, and the co-design Center for Efficient Exascale Discretizations (CEED) in the Exascale computing Project. ===Denoising=== Iterative LOBPCG-based approximate low-pass filter can be used for denoising; see, e.g., to accelerate total variation denoising. === Image segmentation=== Hypre implementation of LOBPCG with multigrid preconditioning has been applied to image segmentation in via spectral graph partitioning using the graph Laplacian for the bilateral filter. ===Data mining=== Software packages scikit-learn and Megaman use LOBPCG to scale spectral clustering and manifold learning via Laplacian eigenmaps to large data sets. NVIDIA has implemented LOBPCG in its nvGRAPH library introduced in CUDA 8. ==References== ==External links== *LOBPCG in MATLAB *LOBPCG in Octave *LOBPCG in SciPy *LOBPCG in Java at Google Code *LOBPCG in Block Locally Optimal Preconditioned Eigenvalue Xolvers (BLOPEX) at GitHub and archived at Google Code Category:Numerical linear algebra Category:Scientific simulation software ",
    "title": "LOBPCG"
}