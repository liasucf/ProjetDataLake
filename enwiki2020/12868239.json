{
    "id": "12868239",
    "text": "The topic of heteroscedasticity-consistent (HC) standard errors arises in statistics and econometrics in the context of linear regression and time series analysis. These are also known as Eicker\u2013Huber\u2013White standard errors (also Huber\u2013White standard errors or White standard errors), to recognize the contributions of Friedhelm Eicker, Peter J. Huber, and Halbert White. In regression and time-series modelling, basic forms of models make use of the assumption that the errors or disturbances ui have the same variance across all observation points. When this is not the case, the errors are said to be heteroscedastic, or to have heteroscedasticity, and this behaviour will be reflected in the residuals \\widehat{u}_i estimated from a fitted model. Heteroscedasticity-consistent standard errors are used to allow the fitting of a model that does contain heteroscedastic residuals. The first such approach was proposed by Huber (1967), and further improved procedures have been produced since for cross-sectional data, time-series data and GARCH estimation. Heteroscedasticity-consistent standard errors that differ from classical standard errors is an indicator of model misspecification. This misspecification is not fixed by merely replacing the classical with heteroscedasticity-consistent standard errors; for all but a few quantities of interest, the misspecification may lead to bias. In most situations, the problem should be found and fixed. Other types of standard error adjustments, such as clustered standard errors, may be considered as extensions to HC standard errors. == History == Heteroscedasticity-consistent standard errors are introduced by Friedhelm Eicker, and popularized in econometrics by Halbert White. ==Problem== Assume that we are studying the linear regression model : Y = X \\beta + U, \\, where X is the vector of explanatory variables and \u03b2 is a k \u00d7 1 column vector of parameters to be estimated. The ordinary least squares (OLS) estimator is : \\widehat \\beta_\\text{OLS} = (\\mathbb{X}' \\mathbb{X})^{-1} \\mathbb{X}' \\mathbb{Y}. \\, where \\mathbb{X} denotes the matrix of stacked X_i' values observed in the data. If the sample errors have equal variance \u03c32 and are uncorrelated, then the least-squares estimate of \u03b2 is BLUE (best linear unbiased estimator), and its variance is easily estimated with : v_\\text{OLS}\\left[\\widehat\\beta_\\text{OLS}\\right] = s^2 (\\mathbb{X}'\\mathbb{X})^{-1}, \\quad s^2 = \\frac{\\sum_i \\widehat u_i^2}{n-k} where \\widehat u_i = Y_i - X_i \\widehat \\beta_\\text{OLS} are the regression residuals. When the assumptions of \\operatorname E[uu'] = \\sigma^2 I_n are violated, the OLS estimator loses its desirable properties. Indeed, : V\\left[\\widehat\\beta_\\text{OLS}\\right] = V[ (\\mathbb{X}'\\mathbb{X})^{-1} \\mathbb{X}'\\mathbb{Y}] = (\\mathbb{X}'\\mathbb{X})^{-1} \\mathbb{X}' \\Sigma \\mathbb{X} (\\mathbb{X}'\\mathbb{X})^{-1} where \\Sigma = V[u]. While the OLS point estimator remains unbiased, it is not \"best\" in the sense of having minimum mean square error, and the OLS variance estimator v_\\text{OLS} \\left[ \\widehat\\beta_\\text{OLS} \\right] does not provide a consistent estimate of the variance of the OLS estimates. For any non-linear model (for instance logit and probit models), however, heteroscedasticity has more severe consequences: the maximum likelihood estimates of the parameters will be biased (in an unknown direction), as well as inconsistent (unless the likelihood function is modified to correctly take into account the precise form of heteroscedasticity). As pointed out by Greene, \u201csimply computing a robust covariance matrix for an otherwise inconsistent estimator does not give it redemption.\u201d ==Solution== If the regression errors u_i are independent, but have distinct variances \u03c3i2, then \\Sigma = \\operatorname{diag}(\\sigma_1^2, \\ldots, \\sigma_n^2) which can be estimated with \\widehat\\sigma_i^2 = \\widehat u_i^2. This provides White's (1980) estimator, often referred to as HCE (heteroscedasticity-consistent estimator): : \\begin{align} v_\\text{HCE} \\left[ \\widehat\\beta_\\text{OLS} \\right] &= \\frac{1}{n} \\left(\\frac{1}{n} \\sum_i X_i X_i' \\right)^{-1} \\left(\\frac{1}{n} \\sum_i X_i X_i' \\widehat{u}_i^2 \\right) \\left(\\frac{1}{n} \\sum_i X_i X_i' \\right)^{-1} \\\\\\ &= ( \\mathbb{X}' \\mathbb{X} )^{-1} ( \\mathbb{X}' \\operatorname{diag}(\\widehat u_1^2, \\ldots, \\widehat u_n^2) \\mathbb{X} ) ( \\mathbb{X}' \\mathbb{X})^{-1}, \\end{align} where as above \\mathbb{X} denotes the matrix of stacked X_i' values from the data. The estimator can be derived in terms of the generalized method of moments (GMM). Note that also often discussed in the literature (including in White's paper itself) is the covariance matrix \\widehat\\Omega_n of the \\sqrt{n}-consistent limiting distribution: : \\sqrt{n}(\\widehat\\beta_n - \\beta) \\, \\xrightarrow{d} \\, N(0,\\Omega), where : \\Omega = \\operatorname E[X X']^{-1} \\operatorname{Var}[X u]\\operatorname E[X X']^{-1}, and : \\begin{align} \\widehat\\Omega_n &= \\left(\\frac{1}{n} \\sum_i X_i X_i' \\right)^{-1} \\left(\\frac{1}{n} \\sum_i X_i X_i' \\widehat u_i^2 \\right) \\left(\\frac{1}{n} \\sum_i X_i X_i' \\right)^{-1} \\\\\\ &= n ( \\mathbb{X}' \\mathbb{X} )^{-1} ( \\mathbb{X}' \\operatorname{diag}(\\widehat u_1^2, \\ldots, \\widehat u_n^2) \\mathbb{X} ) ( \\mathbb{X}' \\mathbb{X})^{-1} \\end{align} Thus, : \\widehat\\Omega_n = n \\cdot v_\\text{HCE}[\\widehat\\beta_\\text{OLS}] and : \\widehat\\operatorname{Var}[X u] = \\frac{1}{n} \\sum_i X_i X_i' \\widehat u_i^2 = \\frac{1}{n} \\mathbb{X}' \\operatorname{diag}(\\widehat u_1^2, \\ldots, \\widehat u_n^2) \\mathbb{X}. Precisely which covariance matrix is of concern is a matter of context. Alternative estimators have been proposed in MacKinnon & White (1985) that correct for unequal variances of regression residuals due to different leverage. Unlike the asymptotic White's estimator, their estimators are unbiased when the data are homoscedastic. ==See also== *Delta method *Generalized least squares *Generalized estimating equations *Weighted least squares, an alternative formulation *White test \u2014 a test for whether heteroscedasticity is present. *Newey\u2013West estimator *Quasi-maximum likelihood estimate ==Software== * EViews: EViews version 8 offers three different methods for robust least squares: M-estimation (Huber, 1973), S-estimation (Rousseeuw and Yohai, 1984), and MM-estimation (Yohai 1987).http://www.eviews.com/EViews8/ev8ecrobust_n.html * MATLAB: See the `hac` function in the Econometrics toolbox. * Python: The Statsmodel package offers various robust standard error estimates, see statsmodels.regression.linear_model.RegressionResults for further descriptions * R: the `vcovHC()` command from the `sandwich` package.sandwich: Robust Covariance Matrix Estimators * RATS: `robusterrors` option is available in many of the regression and optimization commands (`linreg`, `nlls`, etc.). * Stata: `robust` option applicable in many pseudo-likelihood based procedures.See online help for `_robust` option and `regress` command. * Gretl: the option `--robust` to several estimation commands (such as `ols`) in the context of a cross-sectional dataset produces robust standard errors. ==References== ==Further reading== * * * * * Category:Regression analysis Category:Simultaneous equation methods (econometrics) ",
    "title": "Heteroscedasticity-consistent standard errors"
}