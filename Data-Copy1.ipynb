{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9b5d0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import numpy as np\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a063a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"inter-dataset-metadata\": [physical and logical links - Physical links] must be done in Neo4J \n",
    "#logical (intangible) links highlight similarities between documents from their intrinsic characteristics,\n",
    "#such as common word rate or inherent topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19547d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\", disable=['parser', 'ner'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "470af8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lfurtado\\spark\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\n",
      "C:\\Users\\lfurtado\\spark\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\./bin/spark-submit.cmd\n"
     ]
    }
   ],
   "source": [
    "print(os.environ.get(\"SPARK_HOME\"))\n",
    "print(os.path.join(os.environ.get(\"SPARK_HOME\"), './bin/spark-submit.cmd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a1e240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.conf.SparkConf().setAll([\n",
    "                                   ('spark.executorEnv.OMP_NUM_THREADS', '8'),\n",
    "                                   ('spark.workerEnv.OMP_NUM_THREADS', '8'),\n",
    "                                   ('spark.executorEnv.OPENBLAS_NUM_THREADS', '8'),\n",
    "                                   ('spark.workerEnv.OPENBLAS_NUM_THREADS', '8'),\n",
    "                                   ('spark.executorEnv.MKL_NUM_THREADS', '8'),\n",
    "                                   ('spark.workerEnv.MKL_NUM_THREADS', '8'),\n",
    "                                   ])\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6367d2e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lfurtado\\Anaconda3\\lib\\site-packages\\pymongo\\common.py:771: UserWarning: Unknown option ssl_cert_reqs\n",
      "  warnings.warn(str(exc))\n"
     ]
    }
   ],
   "source": [
    "client = MongoClient(\"mongodb+srv://admin:admin@cluster0.wjk47.mongodb.net/WikepediaMetadatas?ssl=true&ssl_cert_reqs=CERT_NONE\")\n",
    "\n",
    "client.list_database_names()\n",
    "\n",
    "#database \n",
    "db = client[\"WikepediaMetadatas\"]\n",
    "   \n",
    "# Created or Switched to collection \n",
    "# names: GeeksForGeeks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01f5a1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'13205523.json'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##CLEAN UNTIL FILE 38 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c091c645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|████▊                                                                 | 28052/411393 [1:15:20<14:59:07,  7.11it/s]ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lfurtado\\spark\\spark-3.2.1-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"C:\\Users\\lfurtado\\spark\\spark-3.2.1-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\clientserver.py\", line 475, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"C:\\Users\\lfurtado\\Anaconda3\\lib\\socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "  7%|████▊                                                                 | 28052/411393 [1:15:21<17:09:41,  6.20it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos_json\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      7\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124menwiki2020/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(pos_json)\n\u001b[1;32m----> 8\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmultiline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[0;32m      9\u001b[0m     docs \u001b[38;5;241m=\u001b[39m [[w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m word_tokenize(text)] \n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n\u001b[0;32m     11\u001b[0m     bag_of_words \u001b[38;5;241m=\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m docs \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]\n",
      "File \u001b[1;32m~\\spark\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\sql\\readwriter.py:229\u001b[0m, in \u001b[0;36mDataFrameReader.json\u001b[1;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, allowNonNumericNumbers, modifiedBefore, modifiedAfter)\u001b[0m\n\u001b[0;32m    227\u001b[0m     path \u001b[38;5;241m=\u001b[39m [path]\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m--> 229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[1;32m~\\spark\\spark-3.2.1-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32m~\\spark\\spark-3.2.1-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[0;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[1;32m~\\spark\\spark-3.2.1-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\clientserver.py:475\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 475\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    476\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[0;32m    478\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Collection = db[\"Metadatas_Raw\"]\n",
    "\n",
    "path_to_json = 'enwiki2020/'\n",
    "index = 1\n",
    "for pos_json in tqdm(os.listdir(path_to_json)):\n",
    "    if pos_json.endswith('.json'):\n",
    "        path = 'enwiki2020/' + str(pos_json)\n",
    "        df = spark.read.option(\"multiline\",\"true\").json(path).toPandas()\n",
    "        docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in list(df['text'])]\n",
    "        bag_of_words = [item for sublist in docs for item in sublist]\n",
    "        word_fd = nltk.FreqDist(bag_of_words).most_common(5)\n",
    "        word_list = [x[0] for x in word_fd]\n",
    "\n",
    "        data = {\n",
    "          \"id\": index, \n",
    "            #Technical metadata\n",
    "           \"intra-dataset-metadata\": [ {\n",
    "            \"properties\": [\n",
    "            {\"file_name\": pos_json,\n",
    "            \"file_size\" : str(round(os.path.getsize(path)/(1024))) + \" MB\",\n",
    "            \"creation_date\": datetime.fromtimestamp(os.path.getctime(path)).strftime('%Y-%m-%d %H:%M:%S') ,\n",
    "            \"sensitivity_level\": \"low\",\n",
    "             \"title\": df['title'][0], \n",
    "            \"document_type\" : \"text\", \n",
    "             \"language\": \"english\", \n",
    "             \"number_of_words\" : list(df['text'].apply(lambda x: len(x.split())))[0]\n",
    "              }], \n",
    "             \"previzualization\": [ {\n",
    "            #previsualization metadata\n",
    "             \"most_common_words\" : word_list , \n",
    "             \"keywords\" : word_list \n",
    "             }],\n",
    "            \"presentation\": [ {\n",
    "                \"transformation\" : \"Original version\",\n",
    "                \"presentation\" : \"Classic presentation\"\n",
    "            }]\n",
    "               #tfid-cleaned\n",
    "           }]\n",
    "          \n",
    "        }\n",
    "        Collection.insert_one(data)\n",
    "        index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e29c1046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stopwords_en = set(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec353042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(msg):\n",
    "    #removing pontuation\n",
    "    No_Punctuation = [char if char not in string.punctuation else ' ' for char in msg ]\n",
    "    sentence = ''.join(No_Punctuation)\n",
    "    #remove all non latin caracters\n",
    "    sentence = re.sub(r'[^\\x00-\\x7f]',r'', sentence)\n",
    "    #removing digits\n",
    "    sentence = re.sub(\"\\S*\\d+\\S*\", \"\", sentence)\n",
    "    #### Word tokenization is the process of splitting up “sentences” into “words”\n",
    "    #sentence = nltk.word_tokenize(sentence)\n",
    "    #Lemmatizing the words\n",
    "    sentence = nlp(sentence)\n",
    "    #lemmetazer = WordNetLemmatizer()\n",
    "    return \" \".join([token.lemma_ for token in sentence if token not in stopwords_en])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a91e4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = df['text'].apply(lambda x:cleanup_text(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a38d41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                            | 4/601 [48:50<122:11:30, 736.84s/it]"
     ]
    }
   ],
   "source": [
    "path_to_json = 'enwiki2020/'\n",
    "for pos_json in tqdm(os.listdir(path_to_json)[4:]):\n",
    "    if pos_json.endswith('.json'):\n",
    "        path = 'enwiki2020/' + str(pos_json)\n",
    "        df = spark.read.option(\"multiline\",\"true\").json(path).toPandas()\n",
    "        df['text_clean'] = df['text'].apply(lambda x:cleanup_text(x))\n",
    "        df_clean = df[['id', 'text_clean', 'title' ]]\n",
    "        df_clean.to_json(r'enwiki2020-clean/' + str(pos_json.replace('.json', ''))+'-clean.json', orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec940477",
   "metadata": {},
   "outputs": [],
   "source": [
    "Collection = db[\"Metadatas_Cleaned\"]\n",
    "\n",
    "path_to_json = 'enwiki2020-clean/'\n",
    "index = 1\n",
    "for pos_json in tqdm(os.listdir(path_to_json)):\n",
    "    if pos_json.endswith('.json'):\n",
    "        path = 'enwiki2020-clean/' + str(pos_json)\n",
    "        df = spark.read.option(\"multiline\",\"true\").json(path).toPandas()\n",
    "        docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in list(df['text_clean'])]\n",
    "        bag_of_words = [item for sublist in docs for item in sublist]\n",
    "        word_fd = nltk.FreqDist(bag_of_words).most_common(10)\n",
    "        word_list = [x[0] for x in word_fd]\n",
    "\n",
    "        data = {\n",
    "          \"id\": index, \n",
    "            #Technical metadata\n",
    "           \"intra-dataset-metadata\": [ {\n",
    "            \"properties\": [\n",
    "            {\"file_name\": pos_json,\n",
    "            \"file_size\" : str(round(os.path.getsize(path)/(1024))) + \" MB\",\n",
    "            \"creation_date\": datetime.fromtimestamp(os.path.getctime(path)).strftime('%Y-%m-%d %H:%M:%S') ,\n",
    "            \"sensitivity_level\": \"low\",\n",
    "             \"title\": df['title'][0], \n",
    "            \"document_type\" : \"text\", \n",
    "             \"language\": \"english\", \n",
    "             \"number_of_words\" : list(df['text'].apply(lambda x: len(x.split())))[0]\n",
    "              }], \n",
    "             \"previzualization\": [ {\n",
    "            #previsualization metadata\n",
    "             \"most_common_words\" : word_list , \n",
    "             \"keywords\" : word_list \n",
    "             }],\n",
    "            \"presentation\": [ {\n",
    "                \"transformation\" : \"Lemmatized version\",\n",
    "                \"presentation\" : \"Classic presentation\"\n",
    "            }]\n",
    "               #tfid-cleaned\n",
    "           }]\n",
    "          \n",
    "        }\n",
    "        Collection.insert_one(data)\n",
    "        index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87f0de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_json = 'enwiki2020-clean/'\n",
    "for pos_json in tqdm(os.listdir(path_to_json)):\n",
    "    if pos_json.endswith('.json'):\n",
    "        path = 'enwiki2020-clean/' + str(pos_json)\n",
    "        df = spark.read.option(\"multiline\",\"true\").json(path).toPandas()\n",
    "\n",
    "        vectorizer = TfidfVectorizer(min_df=2, max_df = 0.8, max_features= 1000)\n",
    "        vectors = vectorizer.fit_transform(df['text_clean'])\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        dense = vectors.todense()\n",
    "        #denselist = dense.tolist()\n",
    "        tfid = pd.DataFrame(dense, columns=feature_names)\n",
    "        tfid.to_json(r'enwiki2020-tfidf/' + str(pos_json.removesuffix('.json'))+'-tfidf.json', orient='records')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c9901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Collection = db[\"Metadatas_TF\"]\n",
    "\n",
    "path_to_json = 'enwiki2020-tfidf/'\n",
    "index = 1\n",
    "for pos_json in tqdm(os.listdir(path_to_json)):\n",
    "    if pos_json.endswith('.json'):\n",
    "        path = 'enwiki2020-tfidf/' + str(pos_json)\n",
    "        tfid = spark.read.option(\"multiline\",\"true\").json(path).toPandas()\n",
    "        word_list = list(tfid.columns[list(np.argsort(tfid.sum(axis=0))[::-1][:10])])\n",
    "        data = {\n",
    "          \"id\": index, \n",
    "            #Technical metadata\n",
    "           \"intra-dataset-metadata\": [ {\n",
    "            \"properties\": [\n",
    "            {\"file_name\": pos_json,\n",
    "            \"file_size\" : str(round(os.path.getsize(path)/(1024))) + \" MB\",\n",
    "            \"creation_date\": datetime.fromtimestamp(os.path.getctime(path)).strftime('%Y-%m-%d %H:%M:%S') ,\n",
    "            \"sensitivity_level\": \"low\",\n",
    "            \"document_type\" : \"numeric\", \n",
    "             \"language\": \"english\", \n",
    "             \"previzualization\": [ {\n",
    "            #previsualization metadata\n",
    "             \"most_common_words\" : word_list , \n",
    "             \"keywords\" : word_list \n",
    "             }],\n",
    "            \"presentation\": [ {\n",
    "                \"transformation\" : \"Lemmatized version\",\n",
    "                \"presentation\" : \"TFIDF Vector\"\n",
    "            }]\n",
    "               #tfid-cleaned\n",
    "           }]\n",
    "          \n",
    "        }\n",
    "        Collection.insert_one(data)\n",
    "        index = index + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
