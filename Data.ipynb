{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e9b5d0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import numpy as np\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f2e2b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\", disable=['parser', 'ner'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "470af8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lfurtado\\spark\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\n",
      "C:\\Users\\lfurtado\\spark\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\./bin/spark-submit.cmd\n"
     ]
    }
   ],
   "source": [
    "print(os.environ.get(\"SPARK_HOME\"))\n",
    "print(os.path.join(os.environ.get(\"SPARK_HOME\"), './bin/spark-submit.cmd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4a1e240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.conf.SparkConf().setAll([\n",
    "                                   ('spark.executorEnv.OMP_NUM_THREADS', '8'),\n",
    "                                   ('spark.workerEnv.OMP_NUM_THREADS', '8'),\n",
    "                                   ('spark.executorEnv.OPENBLAS_NUM_THREADS', '8'),\n",
    "                                   ('spark.workerEnv.OPENBLAS_NUM_THREADS', '8'),\n",
    "                                   ('spark.executorEnv.MKL_NUM_THREADS', '8'),\n",
    "                                   ('spark.workerEnv.MKL_NUM_THREADS', '8'),\n",
    "                                   ])\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6367d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(\"mongodb+srv://admin:admin@cluster0.wjk47.mongodb.net/WikepediaMetadatas?ssl=true&ssl_cert_reqs=CERT_NONE\")\n",
    "\n",
    "client.list_database_names()\n",
    "\n",
    "#database \n",
    "db = client[\"WikepediaMetadatas\"]\n",
    "   \n",
    "# Created or Switched to collection \n",
    "# names: GeeksForGeeks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c091c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "Collection = db[\"Metadatas_Raw\"]\n",
    "\n",
    "path_to_json = 'enwiki20201020/'\n",
    "index = 1\n",
    "for pos_json in tqdm(os.listdir(path_to_json)):\n",
    "    if pos_json.endswith('.json'):\n",
    "        path = 'enwiki20201020/' + str(pos_json)\n",
    "        df = spark.read.option(\"multiline\",\"true\").json(path).toPandas()\n",
    "        docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in list(df['text'])]\n",
    "        bag_of_words = [item for sublist in docs for item in sublist]\n",
    "        word_fd = nltk.FreqDist(bag_of_words).most_common(10)\n",
    "        word_list = [x[0] for x in word_fd]\n",
    "\n",
    "        data = {\n",
    "          \"Id\": index, \n",
    "            #Technical metadata\n",
    "           \"metadata\": [\n",
    "            {\"file_name\": pos_json,\n",
    "            \"file_size\" : str(round(os.path.getsize(path)/(1024*1024))) + \" KB\",\n",
    "            \"creation_date\": datetime.fromtimestamp(os.path.getctime(path)).strftime('%Y-%m-%d %H:%M:%S') ,\n",
    "            \"sensitivity_level\": \"low\",\n",
    "            \"document_type\" : \"text\",\n",
    "             \"transformation_version\" : \"raw\",\n",
    "            #previsualization metadata\n",
    "            \"number_of_texts\" : len(df),\n",
    "            #\"number_of_words\" : list(df['text'].apply(lambda x: len(x.split())))\n",
    "             \"most_common_words\" : word_list , \n",
    "             }\n",
    "               #tfid-cleaned\n",
    "          ]\n",
    "        }\n",
    "        Collection.insert_one(data)\n",
    "        index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "899e81f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = 'enwiki20201020/00c2bfc7-57db-496e-9d5c-d62f8d8119e3.json'\n",
    "df = spark.read.option(\"multiline\",\"true\").json(path).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e82b4b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7751000</td>\n",
       "      <td>M-137 was a state trunkline highway in the US ...</td>\n",
       "      <td>M-137 (Michigan highway)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7751001</td>\n",
       "      <td>In sociology, dynamic density refers to the co...</td>\n",
       "      <td>Dynamic density</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7751042</td>\n",
       "      <td>Bert Robert Shepard (June 20, 1920 – June 16, ...</td>\n",
       "      <td>Bert Shepard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7751048</td>\n",
       "      <td>Marc Fein (born Marc Alan Fein October 21, 196...</td>\n",
       "      <td>Marc Fein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7751062</td>\n",
       "      <td>Ghelamco Arena panorama indoor. The Ghelamco A...</td>\n",
       "      <td>Ghelamco Arena</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>7853026</td>\n",
       "      <td>State Highway 204 (SH 204) is a Texas state hi...</td>\n",
       "      <td>Texas State Highway 204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>7853030</td>\n",
       "      <td>Shrimp chips may refer to: *Prawn cracker, an ...</td>\n",
       "      <td>Shrimp chips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>7853033</td>\n",
       "      <td>Martin Hhaway Sulle (born 28 December 1982) is...</td>\n",
       "      <td>Martin Sulle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>7853047</td>\n",
       "      <td>Krak des Chevaliers was built during the 12th ...</td>\n",
       "      <td>List of Crusader castles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>7853061</td>\n",
       "      <td>Wardner is a side-scrolling platform game deve...</td>\n",
       "      <td>Wardner (video game)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9982 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               text  \\\n",
       "0     7751000  M-137 was a state trunkline highway in the US ...   \n",
       "1     7751001  In sociology, dynamic density refers to the co...   \n",
       "2     7751042  Bert Robert Shepard (June 20, 1920 – June 16, ...   \n",
       "3     7751048  Marc Fein (born Marc Alan Fein October 21, 196...   \n",
       "4     7751062  Ghelamco Arena panorama indoor. The Ghelamco A...   \n",
       "...       ...                                                ...   \n",
       "9977  7853026  State Highway 204 (SH 204) is a Texas state hi...   \n",
       "9978  7853030  Shrimp chips may refer to: *Prawn cracker, an ...   \n",
       "9979  7853033  Martin Hhaway Sulle (born 28 December 1982) is...   \n",
       "9980  7853047  Krak des Chevaliers was built during the 12th ...   \n",
       "9981  7853061  Wardner is a side-scrolling platform game deve...   \n",
       "\n",
       "                         title  \n",
       "0     M-137 (Michigan highway)  \n",
       "1              Dynamic density  \n",
       "2                 Bert Shepard  \n",
       "3                    Marc Fein  \n",
       "4               Ghelamco Arena  \n",
       "...                        ...  \n",
       "9977   Texas State Highway 204  \n",
       "9978              Shrimp chips  \n",
       "9979              Martin Sulle  \n",
       "9980  List of Crusader castles  \n",
       "9981      Wardner (video game)  \n",
       "\n",
       "[9982 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e29c1046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stopwords_en = set(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ec353042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(msg):\n",
    "    #removing pontuation\n",
    "    No_Punctuation = [char if char not in string.punctuation else ' ' for char in msg ]\n",
    "    sentence = ''.join(No_Punctuation)\n",
    "    #remove all non latin caracters\n",
    "    sentence = re.sub(r'[^\\x00-\\x7f]',r'', sentence)\n",
    "    #removing digits\n",
    "    sentence = re.sub(\"\\S*\\d+\\S*\", \"\", sentence)\n",
    "    #### Word tokenization is the process of splitting up “sentences” into “words”\n",
    "    #sentence = nltk.word_tokenize(sentence)\n",
    "    #Lemmatizing the words\n",
    "    sentence = nlp(sentence)\n",
    "    #lemmetazer = WordNetLemmatizer()\n",
    "    return \" \".join([token.lemma_ for token in sentence if token not in stopwords_en])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80a5c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = df['text'].apply(lambda x:cleanup_text(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5a38d41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/605 [18:11<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'removesuffix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [74]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_clean\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x:cleanup_text(x))\n\u001b[0;32m      7\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_clean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m ]]\n\u001b[1;32m----> 8\u001b[0m df_clean\u001b[38;5;241m.\u001b[39mto_json(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menwiki-clean/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mpos_json\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremovesuffix\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-clean.json\u001b[39m\u001b[38;5;124m'\u001b[39m, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'removesuffix'"
     ]
    }
   ],
   "source": [
    "path_to_json = 'enwiki20201020/'\n",
    "for pos_json in tqdm(os.listdir(path_to_json)):\n",
    "    if pos_json.endswith('.json'):\n",
    "        path = 'enwiki20201020/' + str(pos_json)\n",
    "        df = spark.read.option(\"multiline\",\"true\").json(path).toPandas()\n",
    "        df['text_clean'] = df['text'].apply(lambda x:cleanup_text(x))\n",
    "        df_clean = df[['id', 'text_clean', 'title' ]]\n",
    "        df_clean.to_json(r'enwiki-clean/' + str(pos_json.removesuffix('.json'))+'-clean.json', orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a3422",
   "metadata": {},
   "outputs": [],
   "source": [
    "Collection = db[\"Metadatas_Cleaned\"]\n",
    "\n",
    "path_to_json = 'enwiki-clean/'\n",
    "index = 1\n",
    "for pos_json in tqdm(os.listdir(path_to_json)):\n",
    "    if pos_json.endswith('.json'):\n",
    "        path = 'enwiki-clean/' + str(pos_json)\n",
    "        df = spark.read.option(\"multiline\",\"true\").json(path).toPandas()\n",
    "        docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in list(df['text_clean'])]\n",
    "        bag_of_words = [item for sublist in docs for item in sublist]\n",
    "        word_fd = nltk.FreqDist(bag_of_words).most_common(10)\n",
    "        word_list = [x[0] for x in word_fd]\n",
    "\n",
    "        data = {\n",
    "          \"Id\": index, \n",
    "            #Technical metadata\n",
    "           \"metadata\": [\n",
    "            {\"file_name\": pos_json,\n",
    "            \"file_size\" : str(round(os.path.getsize(path)/(1024*1024))) + \" KB\",\n",
    "            \"creation_date\": datetime.fromtimestamp(os.path.getctime(path)).strftime('%Y-%m-%d %H:%M:%S') ,\n",
    "            \"sensitivity_level\": \"low\",\n",
    "            \"document_type\" : \"text\",\n",
    "             \"transformation_version\" : \"cleaned\",\n",
    "            #previsualization metadata\n",
    "            \"number_of_texts\" : len(df),\n",
    "            #\"number_of_words\" : list(df['text'].apply(lambda x: len(x.split())))\n",
    "             \"most_common_words\" : word_list , \n",
    "             }\n",
    "               #tfid-cleaned\n",
    "          ]\n",
    "        }\n",
    "        Collection.insert_one(data)\n",
    "        index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87f0de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_json = 'enwiki-clean/'\n",
    "for pos_json in tqdm(os.listdir(path_to_json)):\n",
    "    if pos_json.endswith('.json'):\n",
    "        path = 'enwiki-clean/' + str(pos_json)\n",
    "        df = spark.read.option(\"multiline\",\"true\").json(path).toPandas()\n",
    "\n",
    "        vectorizer = TfidfVectorizer(min_df=2, max_df = 0.8, max_features= 1000)\n",
    "        vectors = vectorizer.fit_transform(df['text_clean'])\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        dense = vectors.todense()\n",
    "        #denselist = dense.tolist()\n",
    "        tfid = pd.DataFrame(dense, columns=feature_names)\n",
    "        tfid.to_json(r'enwiki-tfidf/' + str(pos_json.removesuffix('.json'))+'-tfidf.json', orient='records')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec32791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Collection = db[\"Metadatas_TF\"]\n",
    "\n",
    "path_to_json = 'enwiki-tfidf/'\n",
    "index = 1\n",
    "for pos_json in tqdm(os.listdir(path_to_json)):\n",
    "    if pos_json.endswith('.json'):\n",
    "        path = 'enwiki-tfidf/' + str(pos_json)\n",
    "        tfid = spark.read.option(\"multiline\",\"true\").json(path).toPandas()\n",
    "        word_list = list(tfid.columns[list(np.argsort(tfid.sum(axis=0))[::-1][:10])])\n",
    "        data = {\n",
    "          \"Id\": index, \n",
    "            #Technical metadata\n",
    "           \"metadata\": [\n",
    "            {\"file_name\": pos_json,\n",
    "            \"file_size\" : str(round(os.path.getsize(path)/(1024*1024))) + \" KB\",\n",
    "            \"creation_date\": datetime.fromtimestamp(os.path.getctime(path)).strftime('%Y-%m-%d %H:%M:%S') ,\n",
    "            \"sensitivity_level\": \"low\",\n",
    "            \"document_type\" : \"numeric\",\n",
    "             \"transformation_version\" : \"tfidf\",\n",
    "            #previsualization metadata\n",
    "            \"number_of_texts\" : len(tfid),\n",
    "            #\"number_of_words\" : list(df['text'].apply(lambda x: len(x.split())))\n",
    "             \"most_common_words\" : word_list , \n",
    "             }\n",
    "               #tfid-cleaned\n",
    "          ]\n",
    "        }\n",
    "        Collection.insert_one(data)\n",
    "        index = index + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
